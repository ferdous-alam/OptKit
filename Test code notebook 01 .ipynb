{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import jacfwd, jacrev\n",
    "from jax import random\n",
    "import numpy as np\n",
    "import sympy as sym\n",
    "\n",
    "\n",
    "class UnconstrainedOptimization:\n",
    "    def __init__(self, cost, direction_type, stepsize_type, x0,\n",
    "                 termination_params, algo_params=None):\n",
    "        self.cost = cost\n",
    "        self.x0 = jnp.array(x0)\n",
    "        self.direction_type = direction_type\n",
    "        self.stepsize_type = stepsize_type\n",
    "        self.algo_params = algo_params\n",
    "        self.termination_params = termination_params\n",
    "\n",
    "    def get_step_size(self, x_current, direction):\n",
    "\n",
    "        if self.stepsize_type == 'minimization_rule':\n",
    "            # create symbolic step size: alpha\n",
    "            alpha = sym.Symbol('alpha')\n",
    "            # convert jax device array to numpy list\n",
    "            x_current = x_current.tolist()\n",
    "            direction = direction.tolist()\n",
    "            sym_vec = []\n",
    "            for d in range(len(direction)):\n",
    "                mm = x_current[d] + alpha * direction[d]\n",
    "                sym_vec.append(mm)\n",
    "            sym_cost = self.cost(sym_vec)\n",
    "            sym_derivative = sym.diff(sym_cost, alpha)\n",
    "            alpha_sym = sym.solveset(sym_derivative, alpha)\n",
    "            alpha_sym = list(alpha_sym)\n",
    "\n",
    "            print(type(alpha_sym[1]) == None)\n",
    "\n",
    "            # only keep the real number by checking data type\n",
    "            i = 0\n",
    "            while True:\n",
    "                condition = type(alpha_sym[i] ** 2) is sym.numbers.Float\n",
    "                if condition is True:\n",
    "                    val = alpha_sym[i]\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            alpha = float(val)\n",
    "\n",
    "        elif self.stepsize_type == 'constant':\n",
    "            alpha = self.algo_params[2]\n",
    "\n",
    "        elif self.stepsize_type == 'armijo_rule':\n",
    "            m = 0\n",
    "            condition = False\n",
    "            while True:\n",
    "                alpha = pow(beta, m) * s\n",
    "                x_next = x_current + alpha * direction\n",
    "                condition = armijo_condition(alpha, sigma, cost, x_current, x_next, direction)\n",
    "\n",
    "                if condition is True:\n",
    "                    break\n",
    "                m += 1\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'Unknown step size type, please input one of the following: minimization_tule, constant, armijo rule')\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def get_gradient_direction(self, x_val):\n",
    "        # get derivative of the cost function\n",
    "        derivative = grad(self.cost)(x_val)\n",
    "\n",
    "        if self.direction_type == 'steepest_descent':\n",
    "            # D in case of steepest descent is identity matrix\n",
    "            D = jnp.identity(len(x_val))\n",
    "            direction = - jnp.matmul(D, derivative)\n",
    "\n",
    "        elif self.direction_type == 'newton_method':\n",
    "            hessian = jacfwd(jacrev(self.cost))(x_val)\n",
    "            D = jnp.linalg.inv(hessian)\n",
    "            direction = - jnp.matmul(D, derivative)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\n",
    "                'Unknown gradient based method, please input one of the following: '\n",
    "                'steepest_descent, newton_method')\n",
    "\n",
    "        return direction\n",
    "\n",
    "    def update(self, x_current, X_opt, error_val, error_cache):\n",
    "\n",
    "        # update states\n",
    "        X_opt.append(np.asarray(x_current, dtype=float))\n",
    "        direction = self.get_gradient_direction(x_current)\n",
    "        step_size = self.get_step_size(x_current, direction)\n",
    "        x_next = x_current + step_size * direction\n",
    "        error_val = x_current - x_next\n",
    "        error_cache.append(error_val)\n",
    "\n",
    "        return x_next, X_opt, error_val, error_cache\n",
    "\n",
    "    def run_algorithm(self):\n",
    "        \"\"\"\n",
    "        Runs the algorithm provided the user with starting and termination conditions,\n",
    "        If the termination condition is \"convergence\" then the algorithm runs until convergence\n",
    "        is achieved upto the desired value, the other termination option is to run the algorithm for\n",
    "        a fixed number of timesteps.\n",
    "        :return: X_opt = a matrix with co-ordinates of all the states obtained from running the algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # get termination conditions from algo_params\n",
    "        termination_type, termination_condition = self.termination_params\n",
    "\n",
    "        x_current = self.x0\n",
    "        X_opt = []\n",
    "        error_val = np.inf\n",
    "        error_cache = []\n",
    "\n",
    "        if termination_type == \"convergence\":\n",
    "            while error_val > termination_condition:\n",
    "                x_next, X_opt, error_val, error_cache = self.update(\n",
    "                    x_current, X_opt, error_val, error_cache)\n",
    "                # update state vector\n",
    "                x_current = x_next\n",
    "\n",
    "        elif termination_type == \"fixed_steps\":\n",
    "            for t in range(termination_condition):\n",
    "                x_next, X_opt, error_val, error_cache = self.update(\n",
    "                    x_current, X_opt, error_val, error_cache)\n",
    "                # update state vector\n",
    "                x_current = x_next\n",
    "\n",
    "        return X_opt, error_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "cost = lambda x: 3*pow(x[0], 2) + pow(x[1], 4)\n",
    "\n",
    "termination_params = ('fixed_steps', 1)\n",
    "x0 = [1.0, -2.0]\n",
    "opt = UnconstrainedOptimization(cost, 'steepest_descent', 'minimization_rule', x0,\n",
    "                                termination_params, algo_params=None)\n",
    "X_opt, error_cache = opt.run_algorithm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
